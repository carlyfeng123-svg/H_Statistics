{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfc59f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import cholesky\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor  # xgboost-ish\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f82033d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 0. Global config\n",
    "# =========================================================\n",
    "\n",
    "RNG = np.random.default_rng(2025)\n",
    "\n",
    "N_REPS_NULL = 3          # repetitions under H0 (N1/N2)\n",
    "N_REPS_ALT = 3           # repetitions under H1 (I1/I2/I4/IMR)\n",
    "N_PERM = 100              # permutations for p-value\n",
    "ALPHA = 0.05\n",
    "\n",
    "GRID_2D = 25             # 25x25 grid for (xj, xk)\n",
    "GRID_1D = 50             # 50 pts for 1D PD/ALE\n",
    "\n",
    "# # sample sizes to try (subset of pdf: {200, 1000, 5000})\n",
    "N_LIST = [200]\n",
    "# dimension setting p ∈ {0.2n, n} just to keep runtime ok\n",
    "P_MODE = [\"0.2n\"]\n",
    "\n",
    "# correlation levels\n",
    "RHO_LIST = [0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c3a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Custom Transformer: Fourier Features\n",
    "# =========================================================\n",
    "class FourierFeats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Add sine/cosine basis features to help linear models capture\n",
    "    nonlinear relationships like sin(pi * x) and cos(pi * x).\n",
    "    Each input x_j becomes [x_j, 2*sin(pi*x_j), 2*cos(pi*x_j)].\n",
    "    \"\"\"\n",
    "    def __init__(self, use_cos=True, scale=2.0):\n",
    "        self.use_cos = use_cos\n",
    "        self.scale = scale\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        s = self.scale * np.sin(np.pi * X)\n",
    "        if self.use_cos:\n",
    "            c = self.scale * np.cos(np.pi * X)\n",
    "            return np.hstack([X, s, c])\n",
    "        return np.hstack([X, s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277031d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Custom Transformer: Fourier Features\n",
    "# =========================================================\n",
    "class FourierFeats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Add sine/cosine basis features to help linear models capture\n",
    "    nonlinear relationships like sin(pi * x) and cos(pi * x).\n",
    "    Each input x_j becomes [x_j, 2*sin(pi*x_j), 2*cos(pi*x_j)].\n",
    "    \"\"\"\n",
    "    def __init__(self, use_cos=True, scale=2.0):\n",
    "        self.use_cos = use_cos\n",
    "        self.scale = scale\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        s = self.scale * np.sin(np.pi * X)\n",
    "        if self.use_cos:\n",
    "            c = self.scale * np.cos(np.pi * X)\n",
    "            return np.hstack([X, s, c])\n",
    "        return np.hstack([X, s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e1f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# model factory\n",
    "# =========================================================\n",
    "\n",
    "def make_models(random_state=0):\n",
    "    return {\n",
    "        \"Random Forest\": RandomForestRegressor(\n",
    "            n_estimators=800,\n",
    "            max_depth=None,\n",
    "            min_samples_leaf=1,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \"HistGB\": HistGradientBoostingRegressor(\n",
    "            max_depth=10,\n",
    "            learning_rate=0.05,\n",
    "            random_state=random_state\n",
    "        ),\n",
    "        \"DNN\": Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"mlp\", MLPRegressor(\n",
    "                hidden_layer_sizes=(512, 256, 128),\n",
    "                activation=\"tanh\",\n",
    "                solver=\"adam\",\n",
    "                learning_rate_init=0.002,\n",
    "                batch_size=64, \n",
    "                max_iter=8000,\n",
    "                early_stopping=True,\n",
    "                validation_fraction=0.2,\n",
    "                n_iter_no_change=50,\n",
    "                tol=1e-6, \n",
    "                random_state=random_state))\n",
    "        ]),\n",
    "        \"Elastic Net\": Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"fourier\", FourierFeats(use_cos=True, scale=2.0)),\n",
    "            (\"enet\", ElasticNet(\n",
    "            alpha=1e-4,\n",
    "            l1_ratio=0.3,\n",
    "            max_iter=50000,\n",
    "            random_state=random_state\n",
    "            ))\n",
    "        ]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3464db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Oracle R^2* and R-panel metrics\n",
    "# =========================================================\n",
    "def oracle_r2_star(f: np.ndarray, y: np.ndarray, eps: np.ndarray | None = None) -> float:\n",
    "    \"\"\"\n",
    "    R2* = 1 - Var(eps)/Var(y); eps defaults to y - f.\n",
    "    \"\"\"\n",
    "    if eps is None:\n",
    "        eps = y - f\n",
    "    vy = np.var(y)\n",
    "    return np.nan if vy == 0 else 1.0 - (np.var(eps) / vy)\n",
    "\n",
    "\n",
    "def _corr2(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Squared Pearson correlation with NaN-guard.\n",
    "    \"\"\"\n",
    "    sa, sb = np.std(a), np.std(b)\n",
    "    if sa == 0 or sb == 0:\n",
    "        return np.nan\n",
    "    return float(np.corrcoef(a, b)[0, 1] ** 2)\n",
    "\n",
    "\n",
    "def r_panel_metrics(\n",
    "    y_tr: np.ndarray,\n",
    "    yhat_tr: np.ndarray,\n",
    "    y_te: np.ndarray,\n",
    "    yhat_te: np.ndarray,\n",
    "    f_te: np.ndarray,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Collect R^2 (train/test) and a few helpful correlation-based diagnostics.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"R2_train\": r2_score(y_tr, yhat_tr),\n",
    "        \"R2_test\": r2_score(y_te, yhat_te),\n",
    "        \"corr2_yhat_y\": _corr2(yhat_te, y_te),\n",
    "        \"corr2_yhat_f\": _corr2(yhat_te, f_te),\n",
    "        \"corr2_f_y\": _corr2(f_te, y_te),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 1. Utilities\n",
    "# =========================================================\n",
    "\n",
    "def make_block_corr_matrix(p, rho=0.3, block_size=10):\n",
    "    \"\"\"\n",
    "    Build a block correlation matrix Sigma (p x p) with rho inside blocks,\n",
    "    identity across blocks. PDF: \"block correlation (block size 10–20), ρ in {0.1,...}\" :contentReference[oaicite:8]{index=8}\n",
    "    \"\"\"\n",
    "    Sigma = np.eye(p)\n",
    "    for start in range(0, p, block_size):\n",
    "        end = min(start + block_size, p)\n",
    "        for i in range(start, end):\n",
    "            for j in range(start, end):\n",
    "                if i != j:\n",
    "                    Sigma[i, j] = rho\n",
    "    return Sigma\n",
    "\n",
    "def sample_correlated_normal(n, p, rho=0.3):\n",
    "    Sigma = make_block_corr_matrix(p, rho=rho)\n",
    "    L = cholesky(Sigma)\n",
    "    Z = RNG.standard_normal(size=(n, p))\n",
    "    X = Z @ L.T\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e5c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 2. DGPs\n",
    "# =========================================================\n",
    "\n",
    "def dgp_N1_additive(X, causal_ratio=0.10, max_causal=6):\n",
    "    \"\"\"\n",
    "    Smooth additive, no interaction.\n",
    "    Only a small causal subset has signal.\n",
    "    This makes N1 learnable at moderate n.\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    c = min(max(2, int(np.ceil(causal_ratio * p))), max_causal)\n",
    "    idx = np.arange(c)  # or RNG.choice(p, size=c, replace=False) if you want random\n",
    "    a = RNG.uniform(1.0, 3.0, size=c)\n",
    "    phi = 2 * np.sin(np.pi * X[:, idx])\n",
    "    f = (a * phi).sum(axis=1)\n",
    "    return f\n",
    "\n",
    "def dgp_N2_mixed(X):\n",
    "    # N2: x1 binary, x2 3-level categorical + smooth terms\n",
    "    n, p = X.shape\n",
    "    x1 = (X[:, 0] > 0).astype(float)\n",
    "    # 3-level categorical from X[:,1]\n",
    "    x2_raw = X[:, 1]\n",
    "    x2 = np.digitize(x2_raw, np.quantile(x2_raw, [1/3, 2/3]))  # 0,1,2\n",
    "    alpha1 = 1.0\n",
    "    alpha2B = 0.5\n",
    "    alpha2C = 0.8\n",
    "    f = alpha1 * x1 \\\n",
    "        + alpha2B * (x2 == 1).astype(float) \\\n",
    "        + alpha2C * (x2 == 2).astype(float)\n",
    "    if p > 2:\n",
    "        a = RNG.uniform(0.3, 1.0, size=p-2)\n",
    "        phi = 2 * np.sin(np.pi * X[:, 2:])\n",
    "        f += (a * phi).sum(axis=1)\n",
    "    return f\n",
    "\n",
    "def dgp_I1_linear_times_linear(X, beta=1.0, alpha=1.0):\n",
    "    # I1: f = α(x1 + x2) + β x1 x2\n",
    "    x1 = X[:, 0]\n",
    "    x2 = X[:, 1]\n",
    "    f = alpha * (x1 + x2) + beta * (x1 * x2)\n",
    "    return f\n",
    "\n",
    "def dgp_I2_smooth_interaction(X, beta=1.0, alpha=1.0):\n",
    "    # I2: f = α{sin(pi x1) + cos(pi x2)} + β sin(x1 x2)\n",
    "    x1 = X[:, 0]\n",
    "    x2 = X[:, 1]\n",
    "    f = alpha * (np.sin(np.pi * x1) + np.cos(np.pi * x2)) + beta * np.sin(x1 * x2)\n",
    "    return f\n",
    "\n",
    "def dgp_I4_local_bump(X, beta=1.0, alpha=1.0):\n",
    "    # I4: f = α(x1 + x2) + β exp{-3(x1^2 + x2^2)}\n",
    "    x1 = X[:, 0]\n",
    "    x2 = X[:, 1]\n",
    "    f = alpha * (x1 + x2) + beta * np.exp(-3 * (x1**2 + x2**2))\n",
    "    return f\n",
    "\n",
    "def dgp_IMR_style(X, beta_ratio=0.5):\n",
    "    \"\"\"\n",
    "    fm + fint, and scale to target IMR = R2_int / R2_m :contentReference[oaicite:5]{index=5}\n",
    "    \"\"\"\n",
    "    x1 = X[:, 0]\n",
    "    x2 = X[:, 1]\n",
    "    f_main = np.sin(np.pi * x1) + np.cos(np.pi * x2)\n",
    "    f_int = np.sin(x1 * x2)\n",
    "    var_m = np.var(f_main)\n",
    "    var_i = np.var(f_int)\n",
    "    if var_m == 0 or var_i == 0 or beta_ratio == 0:\n",
    "        return f_main\n",
    "    scale = np.sqrt(beta_ratio * var_m / var_i)\n",
    "    return f_main + scale * f_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc3f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 3. Noise\n",
    "# =========================================================\n",
    "\n",
    "def add_noise_to_target(f, target_R2=0.8):\n",
    "    var_f = np.var(f)\n",
    "    if var_f <= 0:\n",
    "        eps = RNG.normal(0, 1, len(f))\n",
    "        return f + eps\n",
    "    noise_var = var_f * (1 - target_R2) / max(target_R2, 1e-12)\n",
    "    eps = RNG.normal(0, np.sqrt(noise_var), len(f))\n",
    "    return f + eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da39d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 4. PD: 1D & 2D\n",
    "# =========================================================\n",
    "\n",
    "def partial_dependence_2d(model, X_bg, j, k, grid_2d=25):\n",
    "    \"\"\"\n",
    "    2D PD: f_hat_jk(xj, xk) = E_{X- (j,k)}[f(xj, xk, X_- (j,k))]\n",
    "    We'll do brute-force: for each grid pair, replace j,k in background and average.\n",
    "    \"\"\"\n",
    "    xj_vals = np.linspace(np.percentile(X_bg[:, j], 1),\n",
    "                          np.percentile(X_bg[:, j], 99),\n",
    "                          grid_2d)\n",
    "    xk_vals = np.linspace(np.percentile(X_bg[:, k], 1),\n",
    "                          np.percentile(X_bg[:, k], 99),\n",
    "                          grid_2d)\n",
    "    pd_surface = np.zeros((grid_2d, grid_2d))\n",
    "    for a, xj in enumerate(xj_vals):\n",
    "        for b, xk in enumerate(xk_vals):\n",
    "            X_tmp = X_bg.copy()\n",
    "            X_tmp[:, j] = xj\n",
    "            X_tmp[:, k] = xk\n",
    "            y_pred = model.predict(X_tmp)\n",
    "            pd_surface[a, b] = y_pred.mean()\n",
    "    return xj_vals, xk_vals, pd_surface\n",
    "\n",
    "def partial_dependence_1d(model, X_bg, j, grid_1d=50):\n",
    "    xj_vals = np.linspace(np.percentile(X_bg[:, j], 1),\n",
    "                          np.percentile(X_bg[:, j], 99),\n",
    "                          grid_1d)\n",
    "    pd_1d = np.zeros(grid_1d)\n",
    "    for a, xj in enumerate(xj_vals):\n",
    "        X_tmp = X_bg.copy()\n",
    "        X_tmp[:, j] = xj\n",
    "        y_pred = model.predict(X_tmp)\n",
    "        pd_1d[a] = y_pred.mean()\n",
    "    return xj_vals, pd_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500dd6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 5. ALE: 1D & 2D\n",
    "# =========================================================\n",
    "\n",
    "def ale_1d(model, X_bg, j, grid_1d=50):\n",
    "    xj_sorted = np.sort(X_bg[:, j])\n",
    "    z = np.quantile(xj_sorted, np.linspace(0, 1, grid_1d))\n",
    "    effects = np.zeros(grid_1d - 1)\n",
    "    for l in range(grid_1d - 1):\n",
    "        lower, upper = z[l], z[l+1]\n",
    "        if l < grid_1d - 2:\n",
    "            mask = (X_bg[:, j] >= lower) & (X_bg[:, j] < upper)\n",
    "        else:\n",
    "            mask = (X_bg[:, j] >= lower) & (X_bg[:, j] <= upper)\n",
    "        idx = np.where(mask)[0]\n",
    "        if idx.size == 0:\n",
    "            effects[l] = 0.0\n",
    "            continue\n",
    "        X_low = X_bg[idx, :].copy()\n",
    "        X_up  = X_bg[idx, :].copy()\n",
    "        X_low[:, j] = lower\n",
    "        X_up[:, j]  = upper\n",
    "        pred_low = model.predict(X_low)\n",
    "        pred_up  = model.predict(X_up)\n",
    "        effects[l] = (pred_up - pred_low).mean()\n",
    "    ale = np.zeros(grid_1d)\n",
    "    ale[1:] = np.cumsum(effects)\n",
    "    ale = ale - ale.mean()\n",
    "    return z, ale\n",
    "\n",
    "def ale_2d(model, X_bg, j, k, grid_2d=25):\n",
    "    zj = np.quantile(X_bg[:, j], np.linspace(0, 1, grid_2d))\n",
    "    zk = np.quantile(X_bg[:, k], np.linspace(0, 1, grid_2d))\n",
    "    ale_surf = np.zeros((grid_2d, grid_2d))\n",
    "    for b in range(grid_2d - 1):\n",
    "        k_low, k_up = zk[b], zk[b+1]\n",
    "        if b < grid_2d - 2:\n",
    "            mask_k = (X_bg[:, k] >= k_low) & (X_bg[:, k] < k_up)\n",
    "        else:\n",
    "            mask_k = (X_bg[:, k] >= k_low) & (X_bg[:, k] <= k_up)\n",
    "        idx_k = np.where(mask_k)[0]\n",
    "        if idx_k.size == 0:\n",
    "            continue\n",
    "        X_k = X_bg[idx_k, :]\n",
    "        local_j = np.zeros(grid_2d - 1)\n",
    "        for a in range(grid_2d - 1):\n",
    "            j_low, j_up = zj[a], zj[a+1]\n",
    "            if a < grid_2d - 2:\n",
    "                mask_j = (X_k[:, j] >= j_low) & (X_k[:, j] < j_up)\n",
    "            else:\n",
    "                mask_j = (X_k[:, j] >= j_low) & (X_k[:, j] <= j_up)\n",
    "            idx_j = np.where(mask_j)[0]\n",
    "            if idx_j.size == 0:\n",
    "                local_j[a] = 0.0\n",
    "                continue\n",
    "            X_low = X_k[idx_j, :].copy()\n",
    "            X_up  = X_k[idx_j, :].copy()\n",
    "            X_low[:, j] = j_low\n",
    "            X_up[:, j]  = j_up\n",
    "            pred_low = model.predict(X_low)\n",
    "            pred_up  = model.predict(X_up)\n",
    "            local_j[a] = (pred_up - pred_low).mean()\n",
    "        ale_line = np.zeros(grid_2d)\n",
    "        ale_line[1:] = np.cumsum(local_j)\n",
    "        ale_line = ale_line - ale_line.mean()\n",
    "        ale_surf[:, b] = ale_line\n",
    "    ale_surf = ale_surf - ale_surf.mean()\n",
    "    return zj, zk, ale_surf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2f5278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 6. H-statistic\n",
    "# =========================================================\n",
    "\n",
    "def h_statistic_from_pd(pd_2d, pd_j, pd_k, fbar):\n",
    "    \"\"\"\n",
    "    Implements formula:\n",
    "    H^2_{jk} = sum[(f_jk - f_j - f_k + fbar)^2] / sum[(f_jk - fbar)^2]  in [0,1]\n",
    "    \"\"\"\n",
    "    # need to broadcast 1D PDs to 2D\n",
    "    # pd_2d: (A, B)\n",
    "    A, B = pd_2d.shape\n",
    "    pd_j_2d = np.repeat(pd_j.reshape(A, 1), B, axis=1)\n",
    "    pd_k_2d = np.repeat(pd_k.reshape(1, B), A, axis=0)\n",
    "    numer = (pd_2d - pd_j_2d - pd_k_2d + fbar)**2\n",
    "    denom = (pd_2d - fbar)**2\n",
    "    if denom.sum() == 0:\n",
    "        return 0.0\n",
    "    return float(numer.sum() / denom.sum())\n",
    "\n",
    "def compute_H_for_pair(model, X_bg, j, k, grid_2d=25, grid_1d=50, method=\"pd\"):\n",
    "    # This function was incorrectly returning None because the main logic was inside an 'if' block that was skipped.\n",
    "    # The grid_1d parameter is also effectively ignored for consistency with grid_2d in H-statistic calculation.\n",
    "    # use model prediction average as fbar\n",
    "    fbar = model.predict(X_bg).mean()\n",
    "    if method == \"pd\":\n",
    "        xj_vals, xk_vals, pd_2d = partial_dependence_2d(model, X_bg, j, k, grid_2d)\n",
    "        _, pd_j = partial_dependence_1d(model, X_bg, j, grid_2d)  # use same length\n",
    "        _, pd_k = partial_dependence_1d(model, X_bg, k, grid_2d)\n",
    "        return h_statistic_from_pd(pd_2d, pd_j, pd_k, fbar)\n",
    "    elif method == \"ale\":\n",
    "        xj_vals, xk_vals, ale_2d_surf = ale_2d(model, X_bg, j, k, grid_2d)\n",
    "        _, ale_j = ale_1d(model, X_bg, j, grid_2d)\n",
    "        _, ale_k = ale_1d(model, X_bg, k, grid_2d)\n",
    "        return h_statistic_from_pd(ale_2d_surf, ale_j, ale_k, fbar)\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'pd' or 'ale'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12734ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 7. Permutation-based p-value\n",
    "# =========================================================\n",
    "\n",
    "def permute_feature_within_quantile_bins(X_bg, k, n_bins=5, rng=None):\n",
    "    if rng is None:\n",
    "        rng = RNG\n",
    "    X_new = X_bg.copy()\n",
    "    xk = X_bg[:, k]\n",
    "    qs = np.quantile(xk, np.linspace(0, 1, n_bins + 1))\n",
    "    for b in range(n_bins):\n",
    "        if b == n_bins - 1:\n",
    "            mask = (xk >= qs[b]) & (xk <= qs[b+1])\n",
    "        else:\n",
    "            mask = (xk >= qs[b]) & (xk < qs[b+1])\n",
    "        idx = np.where(mask)[0]\n",
    "        if idx.size > 1:\n",
    "            shuffled = idx.copy()\n",
    "            rng.shuffle(shuffled)\n",
    "            X_new[idx, k] = X_bg[shuffled, k]\n",
    "    return X_new\n",
    "\n",
    "def permutation_pvalue_for_pair(model, X_bg, j, k, observed_H,\n",
    "                                B=100, n_bins=5, method=\"pd\"):\n",
    "    \"\"\"\n",
    "    Permute x_k within quantile bins of f_k(x_k) (approximation),\n",
    "    recompute H each time, p = (1 + # perm >= obs) / (1 + B)\n",
    "    \"\"\"\n",
    "    # simple version: permute feature k completely; you can improve to \"within bins\"\n",
    "    H_perm = []\n",
    "    for _ in range(B):\n",
    "        X_perm = permute_feature_within_quantile_bins(X_bg, k, n_bins=n_bins, rng=RNG)\n",
    "        H_b = compute_H_for_pair(model, X_perm, j, k, GRID_2D, GRID_1D, method=method)\n",
    "        H_perm.append(H_b)\n",
    "    H_perm = np.array(H_perm)\n",
    "    pval = (1 + np.sum(H_perm >= observed_H)) / (1 + B)\n",
    "    return pval, H_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaaa5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 8. One experiment: fit models, output PD-H & ALE-H\n",
    "# =========================================================\n",
    "\n",
    "def run_one_experiment(X, y, pair=(0, 1), random_state=0):\n",
    "    \"\"\"\n",
    "    Train all models on (X, y), compute H and permutation p-value for the target pair.\n",
    "    Returns dict of: model -> (H, pval, r2)\n",
    "    \"\"\"\n",
    "    X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=random_state\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_tmp, y_tmp, test_size=0.5, random_state=random_state\n",
    "    )\n",
    "\n",
    "    models = make_models(random_state)\n",
    "    results = {}\n",
    "\n",
    "    # background sample for PD (m = min(2000, n)) per PDF\n",
    "    m = min(2000, X_train.shape[0])\n",
    "    bg_idx = RNG.choice(X_train.shape[0], size=m, replace=False)\n",
    "    X_bg = X_train[bg_idx, :]\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        # PD-H\n",
    "        H_pd = compute_H_for_pair(model, X_bg, pair[0], pair[1],\n",
    "                                  GRID_2D, GRID_1D, method=\"pd\")\n",
    "        pval_pd, _ = permutation_pvalue_for_pair(\n",
    "            model, X_bg, pair[0], pair[1], H_pd,\n",
    "            B=N_PERM, n_bins=5, method=\"pd\"\n",
    "        )\n",
    "\n",
    "        # ALE-H\n",
    "        H_ale = compute_H_for_pair(model, X_bg, pair[0], pair[1],\n",
    "                                   GRID_2D, GRID_1D, method=\"ale\")\n",
    "        pval_ale, _ = permutation_pvalue_for_pair(\n",
    "            model, X_bg, pair[0], pair[1], H_ale,\n",
    "            B=N_PERM, n_bins=5, method=\"ale\"\n",
    "        )\n",
    "\n",
    "        results[name] = {\n",
    "            \"H_pd\": H_pd,\n",
    "            \"pval_pd\": pval_pd,\n",
    "            \"H_ale\": H_ale,\n",
    "            \"pval_ale\": pval_ale,\n",
    "            \"r2\": r2\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302f622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 9. Simulation: Type I (N1, N2)\n",
    "# =========================================================\n",
    "\n",
    "def simulate_type1():\n",
    "    \"\"\"\n",
    "    Under H0 (no interaction), p-values should be ~Uniform(0,1),\n",
    "    so Type-I error ≈ 0.05 at alpha=0.05.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for n in N_LIST:\n",
    "        for p_mode in P_MODE:\n",
    "            p = max(5, int(0.2 * n)) if p_mode == \"0.2n\" else n\n",
    "            for rho in RHO_LIST:\n",
    "                for dgp_name in [\"N1\", \"N2\"]:\n",
    "                    for rep in range(N_REPS_NULL):\n",
    "                        X = sample_correlated_normal(n, p, rho=rho)\n",
    "                        f = (\n",
    "                            dgp_N1_additive(X)\n",
    "                            if dgp_name == \"N1\"\n",
    "                            else dgp_N2_mixed(X)\n",
    "                        )\n",
    "                        y = add_noise_to_target(f, target_R2=0.8)\n",
    "\n",
    "                        res = run_one_experiment(X, y, pair=(0, 1), random_state=rep)\n",
    "                        for model_name, d in res.items():\n",
    "                            records.append({\n",
    "                                \"n\": n,\n",
    "                                \"p\": p,\n",
    "                                \"rho\": rho,\n",
    "                                \"dgp\": dgp_name,\n",
    "                                \"model\": model_name,\n",
    "                                \"H_pd\": d[\"H_pd\"],\n",
    "                                \"pval_pd\": d[\"pval_pd\"],\n",
    "                                \"H_ale\": d[\"H_ale\"],\n",
    "                                \"pval_ale\": d[\"pval_ale\"],\n",
    "                                \"reject_pd\": int(d[\"pval_pd\"] < ALPHA),\n",
    "                                \"reject_ale\": int(d[\"pval_ale\"] < ALPHA),\n",
    "                            })\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807b5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 10. Simulation: Power (I1/I2/I4/IMR)\n",
    "# =========================================================\n",
    "\n",
    "def simulate_power():\n",
    "    \"\"\"\n",
    "    Under H1 (interaction present), we want power = P(reject) to be high.\n",
    "    We'll vary beta to create different interaction strengths.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for n in N_LIST:\n",
    "        for p_mode in P_MODE:\n",
    "            p = max(5, int(0.2 * n)) if p_mode == \"0.2n\" else n\n",
    "            for rho in RHO_LIST:\n",
    "                for dgp_name in [\"I1\", \"I2\", \"I4\", \"IMR\"]:\n",
    "                    for strength in [0.25, 0.5, 1.0]:\n",
    "                        for rep in range(N_REPS_ALT):\n",
    "                            X = sample_correlated_normal(n, p, rho=rho)\n",
    "                            if dgp_name == \"I1\":\n",
    "                                f = dgp_I1_linear_times_linear(X, beta=strength, alpha=1.0)\n",
    "                            elif dgp_name == \"I2\":\n",
    "                                f = dgp_I2_smooth_interaction(X, beta=strength, alpha=1.0)\n",
    "                            elif dgp_name == \"I4\":\n",
    "                                f = dgp_I4_local_bump(X, beta=strength, alpha=1.0)\n",
    "                            else:  # IMR\n",
    "                                f = dgp_IMR_style(X, beta_ratio=strength)\n",
    "\n",
    "                            y = add_noise_to_target(f, target_R2=0.8)\n",
    "                            res = run_one_experiment(X, y, pair=(0, 1), random_state=rep)\n",
    "\n",
    "                            for model_name, d in res.items():\n",
    "                                records.append({\n",
    "                                    \"n\": n,\n",
    "                                    \"p\": p,\n",
    "                                    \"rho\": rho,\n",
    "                                    \"dgp\": dgp_name,\n",
    "                                    \"strength\": strength,\n",
    "                                    \"model\": model_name,\n",
    "                                    \"H_pd\": d[\"H_pd\"],\n",
    "                                    \"pval_pd\": d[\"pval_pd\"],\n",
    "                                    \"H_ale\": d[\"H_ale\"],\n",
    "                                    \"pval_ale\": d[\"pval_ale\"],\n",
    "                                    \"reject_pd\": int(d[\"pval_pd\"] < ALPHA),\n",
    "                                    \"reject_ale\": int(d[\"pval_ale\"] < ALPHA),\n",
    "                                })\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbbb834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 11. Summaries (PD & ALE)\n",
    "# =========================================================\n",
    "\n",
    "def make_type1_table(df_type1):\n",
    "    \"\"\"Aggregate rejection rates by DGP × model for PD and ALE.\"\"\"\n",
    "    tbl_pd = (\n",
    "        df_type1.groupby([\"dgp\", \"model\"])[\"reject_pd\"]\n",
    "        .mean().reset_index().rename(columns={\"reject_pd\": \"type1_pd\"})\n",
    "    )\n",
    "    tbl_ale = (\n",
    "        df_type1.groupby([\"dgp\", \"model\"])[\"reject_ale\"]\n",
    "        .mean().reset_index().rename(columns={\"reject_ale\": \"type1_ale\"})\n",
    "    )\n",
    "    return pd.merge(tbl_pd, tbl_ale, on=[\"dgp\", \"model\"])\n",
    "\n",
    "\n",
    "def make_power_table(df_power):\n",
    "    \"\"\"Aggregate power by DGP × strength × model for PD and ALE.\"\"\"\n",
    "    tbl_pd = (\n",
    "        df_power.groupby([\"dgp\", \"strength\", \"model\"])[\"reject_pd\"]\n",
    "        .mean().reset_index().rename(columns={\"reject_pd\": \"power_pd\"})\n",
    "    )\n",
    "    tbl_ale = (\n",
    "        df_power.groupby([\"dgp\", \"strength\", \"model\"])[\"reject_ale\"]\n",
    "        .mean().reset_index().rename(columns={\"reject_ale\": \"power_ale\"})\n",
    "    )\n",
    "    return pd.merge(tbl_pd, tbl_ale, on=[\"dgp\", \"strength\", \"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab808f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Raw R² table (first 10 rows) ===\n",
      " sim   n  p  rho DGP  beta         model  R2_star  R2_train  R2_test  corr2_yhat_y  corr2_yhat_f  corr2_f_y     room\n",
      "   1 500 10  0.1  N1   0.0 Random Forest 0.798643  0.953164 0.608986      0.628878      0.838780   0.786912 0.189657\n",
      "   1 500 10  0.1  N1   0.0        HistGB 0.798643  0.900477 0.595067      0.613838      0.776611   0.786912 0.203576\n",
      "   1 500 10  0.1  N1   0.0           DNN 0.798643  0.015606 0.014987      0.019495      0.023414   0.786912 0.783657\n",
      "   1 500 10  0.1  N1   0.0   Elastic Net 0.798643  0.809765 0.755114      0.771832      0.980865   0.786912 0.043529\n",
      "   2 500 10  0.1  N1   0.0 Random Forest 0.803708  0.942733 0.581013      0.599288      0.787128   0.800167 0.222696\n",
      "   2 500 10  0.1  N1   0.0        HistGB 0.803708  0.897425 0.608444      0.614773      0.785939   0.800167 0.195264\n",
      "   2 500 10  0.1  N1   0.0           DNN 0.803708  0.545615 0.078078      0.102004      0.131892   0.800167 0.725630\n",
      "   2 500 10  0.1  N1   0.0   Elastic Net 0.803708  0.804928 0.752799      0.755511      0.954987   0.800167 0.050909\n",
      "   3 500 10  0.1  N1   0.0 Random Forest 0.791611  0.947245 0.577667      0.587260      0.735353   0.786468 0.213944\n",
      "   3 500 10  0.1  N1   0.0        HistGB 0.791611  0.900880 0.629506      0.634773      0.804420   0.786468 0.162105\n",
      "\n",
      "=== Summary: median and IQR of R² across simulations ===\n",
      "DGP  beta   n  rho         model  R2_test_median  R2_test_IQR_low  R2_test_IQR_high  R2_star_median  room_median\n",
      " I1   0.5 500  0.1           DNN        0.718669         0.712231          0.729783        0.796037     0.076363\n",
      " I1   0.5 500  0.1   Elastic Net        0.694015         0.667071          0.701788        0.796037     0.109307\n",
      " I1   0.5 500  0.1        HistGB        0.746477         0.732291          0.757451        0.796037     0.051903\n",
      " I1   0.5 500  0.1 Random Forest        0.757612         0.744831          0.779161        0.796037     0.038832\n",
      " I1   1.0 500  0.1           DNN        0.681078         0.667463          0.698768        0.802895     0.116893\n",
      " I1   1.0 500  0.1   Elastic Net        0.508570         0.463187          0.530605        0.802895     0.306631\n",
      " I1   1.0 500  0.1        HistGB        0.667850         0.650780          0.704755        0.802895     0.123006\n",
      " I1   1.0 500  0.1 Random Forest        0.726816         0.691740          0.762075        0.802895     0.072172\n",
      " N1   0.0 500  0.1           DNN        0.018120        -0.023374          0.113833        0.802236     0.785560\n",
      " N1   0.0 500  0.1   Elastic Net        0.765959         0.748165          0.797165        0.802236     0.036329\n",
      " N1   0.0 500  0.1        HistGB        0.622913         0.598411          0.632880        0.802236     0.186770\n",
      " N1   0.0 500  0.1 Random Forest        0.616773         0.578504          0.651048        0.802236     0.179315\n",
      "=== QUICK TEST: H0 (N1 only) ===\n",
      "\n",
      "=== QUICK TEST: H1 (I1, beta=1.0) ===\n",
      "\n",
      "--- QUICK RESULTS ---\n",
      "   setting         model         H_pd  pval_pd     H_ale  pval_ale\n",
      "     H0-N1 Random Forest 2.732641e-04 0.166667  3.901914  0.666667\n",
      "     H0-N1        HistGB 2.231221e-32 0.666667  3.180999  1.000000\n",
      "     H0-N1           DNN 1.047596e-04 0.500000  2.700538  0.333333\n",
      "     H0-N1   Elastic Net 2.993044e-32 0.166667 14.896135  0.666667\n",
      "H1-I1-b1.0 Random Forest 8.460672e-02 0.833333 11.247254  0.166667\n",
      "H1-I1-b1.0        HistGB 2.964399e-03 0.833333  9.716321  0.833333\n",
      "H1-I1-b1.0           DNN 2.640444e-03 0.333333 10.621567  0.833333\n",
      "H1-I1-b1.0   Elastic Net 2.095968e-32 0.333333 19.089754  0.333333\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Main: build the R^2 table\n",
    "# =========================================================\n",
    "\n",
    "def build_r2_table(\n",
    "    n_list=(200,),\n",
    "    p_mode=\"0.2n\",\n",
    "    rho_list=(0.1, 0.3),\n",
    "    sims_per_setting=50,\n",
    "    target_R2=0.8,\n",
    "    dgp_grid=((\"N1\", 0.0), (\"I1\", 0.5), (\"I1\", 1.0)),  # (DGP, beta)\n",
    "    seed=123,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each (n, p, rho, DGP, beta) and each simulation:\n",
    "      1) Generate X, f, y (one y per simulation).\n",
    "      2) Train/test split once per simulation.\n",
    "      3) Fit multiple models on the same split and record:\n",
    "         R2_train, R2_test, R2_star, and room = R2_star - R2_test.\n",
    "    Return a long-form dataframe (one row per model per simulation).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows = []\n",
    "\n",
    "    for n in n_list:\n",
    "        p = 10\n",
    "        # p = max(5, int(0.2 * n)) if p_mode == \"0.2n\" else n\n",
    "\n",
    "        for rho in rho_list:\n",
    "            for (dgp, beta) in dgp_grid:\n",
    "                for sim in range(1, sims_per_setting + 1):\n",
    "                    # one y per simulation\n",
    "                    X = sample_correlated_normal(n, p, rho=rho)\n",
    "\n",
    "                    if dgp == \"N1\":\n",
    "                        f = dgp_N1_additive(X)\n",
    "                    else:  # I1\n",
    "                        f = dgp_I1_linear_times_linear(X, beta=beta, alpha=1.0)\n",
    "\n",
    "                    y = add_noise_to_target(f, target_R2=0.8)\n",
    "\n",
    "                    # single split per simulation\n",
    "                    X_tr, X_te, y_tr, y_te, f_tr, f_te = train_test_split(\n",
    "                        X, y, f, test_size=0.30, random_state=sim\n",
    "                    )\n",
    "\n",
    "                    R2_star = oracle_r2_star(f, y)  # should be close to target_R2\n",
    "\n",
    "                    models = make_models(random_state=sim)\n",
    "                    for name, mdl in models.items():\n",
    "                        mdl.fit(X_tr, y_tr)\n",
    "                        yhat_tr = mdl.predict(X_tr)\n",
    "                        yhat_te = mdl.predict(X_te)\n",
    "\n",
    "                        panel = r_panel_metrics(y_tr, yhat_tr, y_te, yhat_te, f_te)\n",
    "\n",
    "                        rows.append(\n",
    "                            {\n",
    "                                \"sim\": sim,\n",
    "                                \"n\": n,\n",
    "                                \"p\": p,\n",
    "                                \"rho\": rho,\n",
    "                                \"DGP\": dgp,\n",
    "                                \"beta\": beta,\n",
    "                                \"model\": name,\n",
    "                                \"R2_star\": R2_star,\n",
    "                                **panel,\n",
    "                                \"room\": (\n",
    "                                    np.nan\n",
    "                                    if np.isnan(R2_star)\n",
    "                                    else max(0.0, R2_star - panel[\"R2_test\"])\n",
    "                                ),\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "# -----------------------------\n",
    "# Example run\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    df_r2 = build_r2_table(\n",
    "        n_list=(500,),\n",
    "        rho_list=(0.1,),\n",
    "        sims_per_setting=10,\n",
    "        target_R2=0.8, \n",
    "        dgp_grid=((\"N1\", 0.0), (\"I1\", 0.5), (\"I1\", 1.0)),\n",
    "        seed=2025,\n",
    "    )\n",
    "\n",
    "    # --- show first few raw results\n",
    "    print(\"\\n=== Raw R² table (first 10 rows) ===\")\n",
    "    print(df_r2.head(10).to_string(index=False))\n",
    "\n",
    "    # --- show median & IQR summary\n",
    "    summary = (\n",
    "        df_r2.groupby([\"DGP\", \"beta\", \"n\", \"rho\", \"model\"])\n",
    "        .agg(\n",
    "            R2_test_median=(\"R2_test\", \"median\"),\n",
    "            R2_test_IQR_low=(\"R2_test\", lambda x: np.nanpercentile(x, 25)),\n",
    "            R2_test_IQR_high=(\"R2_test\", lambda x: np.nanpercentile(x, 75)),\n",
    "            R2_star_median=(\"R2_star\", \"median\"),\n",
    "            room_median=(\"room\", \"median\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Summary: median and IQR of R² across simulations ===\")\n",
    "    # limit output width for readability\n",
    "    pd.set_option(\"display.width\", 120)\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    print(summary.to_string(index=False))\n",
    "# ============================\n",
    "# QUICK (Type I & Power)\n",
    "# ============================\n",
    "\n",
    "def main_quick():\n",
    "    print(\"=== QUICK TEST: H0 (N1 only) ===\")\n",
    "    global N_PERM\n",
    "    N_PERM = 5  # very few perms for debugging speed\n",
    "\n",
    "    pair = (0, 1)\n",
    "    n = 200\n",
    "    p = max(5, int(0.2 * n))\n",
    "    rho = 0.3\n",
    "\n",
    "    # --- H0 (N1) ---\n",
    "    X = sample_correlated_normal(n, p, rho=rho)\n",
    "    f = dgp_N1_additive(X)\n",
    "    y = add_noise_to_target(f, target_R2=0.8)\n",
    "    res_null = run_one_experiment(X, y, pair=pair, random_state=0)\n",
    "\n",
    "    rows = []\n",
    "    for model_name, d in res_null.items():\n",
    "        rows.append({\n",
    "            \"setting\": \"H0-N1\",\n",
    "            \"model\": model_name,\n",
    "            \"H_pd\": d[\"H_pd\"],\n",
    "            \"pval_pd\": d[\"pval_pd\"],\n",
    "            \"H_ale\": d[\"H_ale\"],\n",
    "            \"pval_ale\": d[\"pval_ale\"],\n",
    "        })\n",
    "\n",
    "    print(\"\\n=== QUICK TEST: H1 (I1, beta=1.0) ===\")\n",
    "    # --- H1 (I1) ---\n",
    "    X = sample_correlated_normal(n, p, rho=rho)\n",
    "    f = dgp_I1_linear_times_linear(X, beta=1.0, alpha=1.0)\n",
    "    y = add_noise_to_target(f, target_R2=0.8)\n",
    "    res_alt = run_one_experiment(X, y, pair=pair, random_state=1)\n",
    "\n",
    "    for model_name, d in res_alt.items():\n",
    "        rows.append({\n",
    "            \"setting\": \"H1-I1-b1.0\",\n",
    "            \"model\": model_name,\n",
    "            \"H_pd\": d[\"H_pd\"],\n",
    "            \"pval_pd\": d[\"pval_pd\"],\n",
    "            \"H_ale\": d[\"H_ale\"],\n",
    "            \"pval_ale\": d[\"pval_ale\"],\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(\"\\n--- QUICK RESULTS ---\")\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_quick()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
